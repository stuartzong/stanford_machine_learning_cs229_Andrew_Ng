{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Newton’s method for computing least squares\n",
    "In this problem, we will prove that if we use Newton’s method solve the least squares\n",
    "optimization problem, then we only need one iteration to converge to $θ^∗$.\n",
    "\n",
    "(a) Find the Hessian of the cost function $J(θ) = \\frac{1}{2}\\sum_{i=1}^{m}(θ^T x^{(i)} − y^{(i)})^2$.\n",
    "\n",
    "(b) Show that the first iteration of Newton’s method gives us $θ^⋆ = (X^T X)^{−1}X^Ty^{hat}$, the\n",
    "solution to our least squares problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hessian matrix is the second partial derivatives, f(x1, x2, ... xn), H is a nxn matrix.\n",
    "$H_{ij}= \\frac{\\partial ^2f}{\\partial x_i\\partial x_j}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function is $J(θ) = \\frac{1}{2}\\sum_{i=1}^{m}(θ^T x^{(i)} − y^{(i)})^2. θ ∈ R^n$ or n+1 dimensional vector if including intercept term $θ_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Some basics about matrix index notation\n",
    "suppose we have two matrices: $A=(A_{ij}) ∈ R^{nxm}$ and $B=(B_{ij}) ∈ R^{mxn}$. The matrix product $C=AB=(C_{ij})=\\sum_{k=1}^mA_{ik}B_{kj} ∈ R^{nxn}$. If we omit the summation and note repeated indices are to be summed, this can simply be written as  $C_{ij} = A_{ik}B_{kj}$.\n",
    "\n",
    "reference is here: https://pdfs.semanticscholar.org/7349/540ff102e701d4b171edbe510c0763ea60b3.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Find the Hessian of the cost function $J(θ) = \\frac{1}{2}\\sum_{i=1}^{m}(θ^T x^{(i)} − y^{(i)})^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's find the second derivatives of J(θ) with respect to θ, which is the H. In this case, $H ∈ R^{nxn}$\n",
    "$J(θ) = \\frac{1}{2}\\sum_{i=1}^{m}(θ^T x^{(i)} − y^{(i)})^2$\n",
    "\n",
    "$g(θ) = \\frac{\\partial J(θ)}{\\partial θ_i}=\\sum_{k=1}^{m}(θ^Tx^{(k)} - y^{(k)})x_i^{(k)}$\n",
    "\n",
    "$H_{ij} = \\frac{\\partial ^2J(θ)}{\\partial θ_i\\partial θ_j}=\\frac{\\partial g(θ)}{\\partial θ_j}=\\sum_{k=1}^m(x_i^{(k)}x_j^{(k)})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let $X ∈ R^{mxn}$ be the input matrix with m training examples and n features (living area, number of bedrooms, whatever)\n",
    "\n",
    "Using index notation, X can be written as $x_{ij}$ and $X^T$ can be written as $x_{ji}$\n",
    "\n",
    "\n",
    "$(X^TX)_{ij} = \\sum_{k=1}^{m}(X^T)_{ik}X_{kj}= \\sum_{k=1}^{m}X_{ki}X_{kj}=\\sum_{k=1}^m(x_i^{(k)}x_j^{(k)})=H_{ij}$\n",
    "\n",
    "Therefore, we have proved that: the Hessian of J(θ) is $H = X^TX$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Show that the first iteration of Newton’s method gives us $θ^⋆ = (X^T X)^{−1}X^Ty$  the solution to our least squares problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### below is linear algebra proof. here we omitted a hundered steps .... wish i know more \n",
    "according to the lecture notes 1:\n",
    "\n",
    "cost function's gradient is: $∇_θJ(θ) = X^TXθ − X^Ty$\n",
    "\n",
    "Newton's update rules: $ θ = θ - H^{-1}\\nabla_θJ(θ)$\n",
    "\n",
    "the Hessian of J(θ) is $H = X^TX$\n",
    "\n",
    "put them all together, the first update look like this:\n",
    "\n",
    "\\begin{align} \\label{eq1}\n",
    "\\begin{split}\n",
    "θ^{(1)} & = θ^{(0)} - H^{-1}\\nabla_θJ(θ^{(0)})\\\\\n",
    " & = θ^{(0)} -(X^TX)^{-1}(X^TXθ^{(0)} − X^Ty\\\\\n",
    " & = θ^{(0)} - θ^{(0)} + (X^TX)^{-1}X^Ty\\\\\n",
    " & = (X^TX)^{-1}X^Ty\n",
    "\\end{split}\n",
    "\\end{align}\n",
    "\n",
    "If you recall, this is exactly the normal equation, which finds the θ minizing the cost function J(θ). Therefore, this proves that whatever you initialize θ, Newton's method can always find the θ which minimizes J(θ) after one iteration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### formating math equations:\n",
    "https://www.overleaf.com/learn/latex/Aligning_equations_with_amsmath#%2FAligning_several_equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### include an example to compare bach gradient descent and Newton's method to see if they converge to the same  $θ^⋆$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this example is from Andrew Ng's courera machine learning course: linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.1101, 5.5277, 8.5186])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([17.592 ,  9.1302, 13.662 ])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(97,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(97,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.loadtxt(os.path.join('data', 'ex1data1.txt'), delimiter=',')\n",
    "X, y = data[:, 0], data[:, 1]\n",
    "m = y.size  \n",
    "X[:3]\n",
    "y[:3]\n",
    "m\n",
    "X.shape\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f1de5f88d68>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Profit in $10,000')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,0,'Population of City in 10,000s')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()  \n",
    "plt.scatter(X, y, marker='.', s=68, c='red')\n",
    "plt.ylabel('Profit in $10,000')\n",
    "plt.xlabel('Population of City in 10,000s')\n",
    "plt.savefig('data_visalization.png', bbox_inches='tight',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column of ones to X. The numpy function stack joins arrays along a given axis. \n",
    "# The first axis (axis=0) refers to rows (training examples) \n",
    "# and second axis (axis=1) refers to columns (features).\n",
    "X = np.stack([np.ones(m), X], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCost(X, y, theta):      \n",
    "    # number of training examples\n",
    "    m = y.size  \n",
    "    J = 0\n",
    "    J = (np.sum((X @ theta - y)**2))/2/m\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X, y, theta, alpha, num_iters):   \n",
    "    m = y.shape[0]  # number of training examples\n",
    "    theta = theta.copy()\n",
    "    J_history = [] \n",
    "    theta_history = []\n",
    "    for i in range(num_iters):        \n",
    "        # must update thetas simultaneously\n",
    "        theta_tmp = []    \n",
    "        for j in range(len(theta)): # partial derivative \n",
    "            gradient = (alpha/m) * np.sum(((X @ theta) - y) * X[:,j])\n",
    "            new_theta = theta[j] - gradient\n",
    "            theta_tmp.append(new_theta)        \n",
    "        theta = theta_tmp\n",
    "        theta_history.append(theta_tmp)\n",
    "        J_history.append(computeCost(X, y, theta))\n",
    "    # add in the intial theta and cost\n",
    "    theta_history = [init_theta.tolist()] + theta_history\n",
    "    J_history = [computeCost(X, y, init_theta)] + J_history\n",
    "    return theta, J_history, theta_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_vectorized(X, y, theta, alpha, num_iters):   \n",
    "    m = y.shape[0]  # number of training examples\n",
    "    theta = theta.copy()\n",
    "    for i in range(num_iters):    \n",
    "        # vectorized implementation\n",
    "        gradients = (alpha/m) * ((X @ theta) - y) @ X\n",
    "        theta -= gradients  \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform gradient descent to learn Theta by minimizing the cost function: $J(\\theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta found by gradient descent: -3.8577, 1.1892\n",
      "Expected theta values (approximately): [-3.6303, 1.1664]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[5, 4],\n",
       " [4.681999350515464, 0.9891407845730922],\n",
       " [4.612858797786178, 0.4551879003217243]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize fitting parameters\n",
    "# theta = np.zeros(2)\n",
    "# theta = np.ones(2)\n",
    "init_theta = np.array([5, 4])\n",
    "# some gradient descent settings\n",
    "iterations = 3000\n",
    "alpha = 0.01\n",
    "\n",
    "theta, J_history, theta_history = gradientDescent(X ,y, init_theta, alpha, iterations)\n",
    "print('Theta found by gradient descent: {:.4f}, {:.4f}'.format(*theta))\n",
    "print('Expected theta values (approximately): [-3.6303, 1.1664]')\n",
    "theta_history[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$θ^⋆ = (X^T X)^{−1}X^Ty$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see Newton's method arrive at the correct theta in one iteration and this is independant to initial θ. $θ^⋆$ is actually the same as the resolution of normal equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.89578088,  1.19303364])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_star = (np.linalg.inv(X.T@X)@X.T)@y\n",
    "theta_star"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
